{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import os\n",
    "from IPython.display import Image as IPImage, display\n",
    "from urllib.parse import urlparse\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Read recipe inputs\n",
    "All_images = dataiku.Folder(\"GzqLfGPX\")\n",
    "all_images = All_images.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataiku\n",
    "# import json\n",
    "# from IPython.display import Image as IPImage, display\n",
    "# from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# # Initialize the Dataiku DSS folder\n",
    "# dataiku_folder = dataiku.Folder(\"GzqLfGPX\")\n",
    "\n",
    "# # Specify the JSON file path within the folder\n",
    "# json_path = 'all_urls_info.json'\n",
    "\n",
    "# # Download the JSON file content from Dataiku DSS folder\n",
    "# with dataiku_folder.get_download_stream(json_path) as json_stream:\n",
    "#     with open(json_path, 'wb') as local_json_file:\n",
    "#         local_json_file.write(json_stream.read())\n",
    "\n",
    "# # Read the JSON file\n",
    "# with open(json_path, 'r') as json_file:\n",
    "#     all_urls_info = json.load(json_file)\n",
    "\n",
    "# # Display images for each URL\n",
    "# for url_info in all_urls_info:\n",
    "#     url = url_info['url']\n",
    "#     image_urls = url_info['images']\n",
    "\n",
    "#     print(f\"URL: {url}\")\n",
    "    \n",
    "#     for i, image_url in enumerate(image_urls):\n",
    "#         try:\n",
    "#             # Check if the image URL is a valid URL\n",
    "#             parsed_url = urlparse(image_url)\n",
    "#             if parsed_url.scheme not in ('http', 'https'):\n",
    "#                 # If not a valid scheme, construct a complete URL using urljoin\n",
    "#                 base_url = urlparse(url)\n",
    "#                 image_url = urljoin(base_url.geturl(), image_url)\n",
    "\n",
    "#             # Display the image\n",
    "#             display(IPImage(url=image_url))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error displaying image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define the image transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define the model architecture (replace TheModelClass with your actual model class)\n",
    "class_names_binary = ['not suitable', 'suitable']\n",
    "\n",
    "model_binary = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_binary.fc.in_features\n",
    "model_binary.fc = nn.Linear(num_ftrs, len(class_names_binary))\n",
    "\n",
    "# Move the model to the device\n",
    "model_binary = model_binary.to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "model_binary.load_state_dict(torch.load('/data/dss-data/managed_folders/MORTGAGEPRICECALCULATOR/Y2CfEnR1/model_unsampled.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_binary.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names_multiclass = ['cox yaxsi', 'orta', 'pis', 'yaxsi']\n",
    "\n",
    "model_multiclass = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_multiclass.fc.in_features\n",
    "model_multiclass.fc = nn.Linear(num_ftrs, len(class_names_multiclass))\n",
    "\n",
    "# Move the model to the device\n",
    "model_multiclass = model_multiclass.to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "model_multiclass.load_state_dict(torch.load('/data/dss-data/managed_folders/MORTGAGEPRICECALCULATOR/PUohl1AF/model_unsampled.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_multiclass.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4 URLs\n",
      "Processing images for URL: https://bina.az/items/4008278\n"
     ]
    }
   ],
   "source": [
    "# Define the main folder\n",
    "main_folder = dataiku.Folder(\"LBuXlV98\")\n",
    "\n",
    "# Create a list to store results\n",
    "classification_results = []\n",
    "\n",
    "\n",
    "\n",
    "# Load the JSON file with image URLs\n",
    "json_path = 'all_urls_info.json'\n",
    "\n",
    "\n",
    "# Download the JSON file content from Dataiku DSS folder\n",
    "with All_images.get_download_stream(json_path) as json_stream:\n",
    "    with open(json_path, 'wb') as local_json_file:\n",
    "        local_json_file.write(json_stream.read())\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_path, 'r') as json_file:\n",
    "    all_urls_info = json.load(json_file)\n",
    "    \n",
    "\n",
    "# Set the batch size for processing URLs\n",
    "batch_size = 100\n",
    "counter = 0  # Initialize the counter\n",
    "\n",
    "urls = [x['url'] for x in all_urls_info]\n",
    "\n",
    "\n",
    "try: \n",
    "    url_df = pd.read_csv(os.path.join(main_folder, 'records_df.csv'))\n",
    "except:\n",
    "    url_df = pd.DataFrame({'URL': urls, 'SCORE': np.nan})\n",
    "    \n",
    "    \n",
    "try:\n",
    "    with open(f'{main_folder.get_path()}/combined_classification_results.json', 'r') as json_file:\n",
    "        classification_results = json.load(json_file)\n",
    "except:\n",
    "    classification_results = []\n",
    "\n",
    "\n",
    "for url_info in all_urls_info:\n",
    "        counter += 1\n",
    "\n",
    "        # Check if the counter is a multiple of 100\n",
    "        if counter % 100 == 0:\n",
    "            print(f\"Processed {counter} URLs\")\n",
    "            print(f\"Processing images for URL: {url}\")\n",
    "\n",
    "        url = url_info['url']\n",
    "        image_urls = url_info['images']\n",
    "\n",
    "        #print(f\"Processing images for URL: {url}\")\n",
    "\n",
    "        url_classification_result = {\n",
    "            'url': url,\n",
    "            'images': []\n",
    "        }\n",
    "\n",
    "        for i, image_url in enumerate(image_urls):\n",
    "            try:\n",
    "                # Check if the image URL is a valid URL\n",
    "                parsed_url = urlparse(image_url)\n",
    "                if parsed_url.scheme not in ('http', 'https'):\n",
    "                    # If not a valid scheme, construct a complete URL using urljoin\n",
    "                    base_url = urlparse(url)\n",
    "                    image_url = urljoin(base_url.geturl(), image_url)\n",
    "\n",
    "                # Generate a unique filename based on the hash of the URL\n",
    "                image_filename = hashlib.md5(image_url.encode()).hexdigest() + '.jpg'\n",
    "\n",
    "                # Download and preprocess image\n",
    "                img = Image.open(requests.get(image_url, stream=True).raw)\n",
    "                input_tensor = preprocess(img)\n",
    "\n",
    "                # Convert the input tensor to a batch tensor\n",
    "                input_batch = torch.unsqueeze(input_tensor, 0)\n",
    "\n",
    "                # Move the input tensor to the device\n",
    "                input_batch = input_batch.to(device)\n",
    "\n",
    "                # Binary model prediction\n",
    "                with torch.no_grad():\n",
    "                    output_binary = model_binary(input_batch)\n",
    "               \n",
    "                # Get the predicted class index for the binary model\n",
    "                _, predicted_index_binary = torch.max(output_binary, 1)\n",
    "\n",
    "                # Map the index to the class name for the binary model\n",
    "                predicted_class_binary = class_names_binary[predicted_index_binary.item()]\n",
    "\n",
    "                if predicted_class_binary == 'suitable':\n",
    "                    # If binary model predicts 'suitable,' proceed with multiclass model\n",
    "                    # Multiclass model prediction\n",
    "                    with torch.no_grad():\n",
    "                        output_multiclass = model_multiclass(input_batch)\n",
    "                 \n",
    "                    # Get the predicted class index for the multiclass model\n",
    "                    _, predicted_index_multiclass = torch.max(output_multiclass, 1)\n",
    "\n",
    "                    # Map the index to the class name for the multiclass model\n",
    "                    predicted_class_multiclass = class_names_multiclass[predicted_index_multiclass.item()]\n",
    "\n",
    "                    url_classification_result['images'].append({\n",
    "                        'image_url': image_url,\n",
    "                        'predicted_class': predicted_class_multiclass\n",
    "                    })\n",
    "                else:\n",
    "                    # If binary model predicts 'not suitable,' save binary result in the list\n",
    "                    url_classification_result['images'].append({\n",
    "                        'image_url': image_url,\n",
    "                        'predicted_class': predicted_class_binary\n",
    "                    })\n",
    "\n",
    "                #plt.show()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Add the classification result for the URL to the overall list\n",
    "        classification_results.append(url_classification_result)\n",
    "\n",
    "\n",
    "\n",
    "        # Save classification results as a single JSON file in the Image_Classification folder\n",
    "        combined_json_path = f'{main_folder.get_path()}/combined_classification_results.json'\n",
    "        with open(combined_json_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(classification_results, json_file, ensure_ascii=False) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Apply mapping to the classification results and add numeric class\n",
    "        class_mapping = {\n",
    "            'cox yaxsi': 4,\n",
    "            'yaxsi': 3,\n",
    "            'orta': 2,\n",
    "            'pis': 1,\n",
    "            'not suitable': 0\n",
    "        }\n",
    "\n",
    "\n",
    "        # Add class_mapping values and total sum to the JSON file\n",
    "        with open(combined_json_path, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        for url_result in data:\n",
    "            numeric_classes = []\n",
    "            for image_result in url_result.get('images', []):\n",
    "                predicted_class = image_result.get('predicted_class')\n",
    "                if predicted_class:\n",
    "                    class_mapping_value = class_mapping.get(predicted_class, -1)\n",
    "                    image_result['class_mapping_value'] = class_mapping_value\n",
    "                    numeric_classes.append(class_mapping_value)\n",
    "\n",
    "            # Calculate 'total_sum' for each URL and add numeric class\n",
    "            # Avoid division by zero\n",
    "            total_sum = sum(numeric_class for numeric_class in numeric_classes if numeric_class != 0)\n",
    "            length = len([1 for numeric_class in numeric_classes if numeric_class != 0])\n",
    "\n",
    "            url_result['vision_score'] = total_sum / length if length != 0 else 0\n",
    "\n",
    "        with open(combined_json_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipe outputs\n",
    "Image_Classification = dataiku.Folder(\"LBuXlV98\")\n",
    "Image_Classification_info = Image_Classification.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import json\n",
    "\n",
    "# Initialize the Dataiku DSS folder\n",
    "dataiku_folder = dataiku.Folder(\"LBuXlV98\")\n",
    "\n",
    "# Specify the JSON file path within the folder\n",
    "json_path = 'combined_classification_results.json'\n",
    "\n",
    "# Download the JSON file content from Dataiku DSS folder\n",
    "with dataiku_folder.get_download_stream(json_path) as json_stream:\n",
    "    with open(json_path, 'wb') as local_json_file:\n",
    "        local_json_file.write(json_stream.read())\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_path, 'r') as json_file:\n",
    "    combined_classification_results = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14812"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_classification_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_df=pd.read_json('combined_classification_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-a52f837c-4a2a-4c19-962c-945d077710b4\" \n",
       "            onclick=\"_export_df('a52f837c-4a2a-4c19-962c-945d077710b4')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(typeof(IPython) === \"undefined\" || !IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"a52f837c-4a2a-4c19-962c-945d077710b4\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>images</th>\n",
       "      <th>vision_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://bina.az/items/3971067</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://bina.az/items/4005977</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>2.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://bina.az/items/3970414</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://bina.az/items/4011976</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://bina.az/items/4000983</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>3.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14807</th>\n",
       "      <td>https://bina.az/items/3962557</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>3.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14808</th>\n",
       "      <td>https://bina.az/items/4011967</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14809</th>\n",
       "      <td>https://bina.az/items/4015932</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14810</th>\n",
       "      <td>https://bina.az/items/3825916</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>3.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14811</th>\n",
       "      <td>https://bina.az/items/3960186</td>\n",
       "      <td>[{'image_url': 'https://bina.azstatic.com/uplo...</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14812 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 url                                             images  vision_score\n",
       "0      https://bina.az/items/3971067  [{'image_url': 'https://bina.azstatic.com/uplo...      3.500000\n",
       "1      https://bina.az/items/4005977  [{'image_url': 'https://bina.azstatic.com/uplo...      2.181818\n",
       "2      https://bina.az/items/3970414  [{'image_url': 'https://bina.azstatic.com/uplo...      3.750000\n",
       "3      https://bina.az/items/4011976  [{'image_url': 'https://bina.azstatic.com/uplo...      2.600000\n",
       "4      https://bina.az/items/4000983  [{'image_url': 'https://bina.azstatic.com/uplo...      3.037037\n",
       "...                              ...                                                ...           ...\n",
       "14807  https://bina.az/items/3962557  [{'image_url': 'https://bina.azstatic.com/uplo...      3.785714\n",
       "14808  https://bina.az/items/4011967  [{'image_url': 'https://bina.azstatic.com/uplo...      1.700000\n",
       "14809  https://bina.az/items/4015932  [{'image_url': 'https://bina.azstatic.com/uplo...      3.666667\n",
       "14810  https://bina.az/items/3825916  [{'image_url': 'https://bina.azstatic.com/uplo...      3.400000\n",
       "14811  https://bina.az/items/3960186  [{'image_url': 'https://bina.azstatic.com/uplo...      3.100000\n",
       "\n",
       "[14812 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# base_url = 'https://bina.az/items/{item_number}'\n",
    "# links = set()\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "#                   '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "# }\n",
    "\n",
    "# url = 'https://bina.az/baki/kiraye/menziller'\n",
    "# page = requests.get(url, headers=headers)\n",
    "# page.raise_for_status()\n",
    "# html = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# # Set the limit for the number of links\n",
    "# limit = 30\n",
    "# count = 0\n",
    "\n",
    "# while True:\n",
    "#     for link in html.find_all('a', href=True):\n",
    "#         href = link['href']\n",
    "#         if '/items/' in href and '=' not in href and any(char.isdigit() for char in href):\n",
    "#             links.add(base_url.format(item_number=href.split('/')[-1]))\n",
    "#             count += 1\n",
    "\n",
    "#             if count >= limit:\n",
    "#                 break\n",
    "\n",
    "#     if count >= limit:\n",
    "#         break\n",
    "\n",
    "#     next_button = html.select_one('.next a')\n",
    "#     if not next_button:\n",
    "#         break\n",
    "\n",
    "#     next_url = 'https://bina.az' + next_button['href']\n",
    "#     page = requests.get(next_url, headers=headers)\n",
    "#     page.raise_for_status()\n",
    "#     html = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# links = list(links)\n",
    "# print(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "# import base64\n",
    "# from urllib.parse import urlparse, urljoin\n",
    "# from IPython.display import Image as IPImage, display\n",
    "# import dataiku\n",
    "# import os\n",
    "\n",
    "# # Keep track of uploaded images to avoid repetition\n",
    "# image_pil = []\n",
    "# processed_urls = set()\n",
    "\n",
    "# for url in links:\n",
    "\n",
    "#     try:\n",
    "#         headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "#         }\n",
    "#         page = requests.get(url, headers=headers)\n",
    "#         page.raise_for_status()\n",
    "#         html = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "#         # Find all image tags\n",
    "#         image_tags = html.find_all('img')\n",
    "\n",
    "#         for i, image_tag in enumerate(image_tags):\n",
    "#             image_url = image_tag.get('src')\n",
    "\n",
    "#             # Check if the image URL is a valid URL\n",
    "#             parsed_url = urlparse(image_url)\n",
    "#             if parsed_url.scheme not in ('http', 'https'):\n",
    "#                 # If not a valid scheme, construct a complete URL using urljoin\n",
    "#                 base_url = urlparse(url)\n",
    "#                 image_url = urljoin(base_url.geturl(), image_url)\n",
    "\n",
    "#             if image_url not in processed_urls:\n",
    "#                 processed_urls.add(image_url)  # Mark the URL as processed\n",
    "\n",
    "#                 if image_url.startswith('data:image'):\n",
    "#                     # Extract base64 data\n",
    "#                     base64_data = image_url.split(',')[1]\n",
    "\n",
    "#                     # Decode base64 and create Image object\n",
    "#                     img_data = base64.b64decode(base64_data)\n",
    "#                     img = Image.open(BytesIO(img_data))\n",
    "#                     if img.format != 'GIF':\n",
    "#                         image_pil.append(img)\n",
    "#                         # Process or save the image as needed\n",
    "\n",
    "#                 else:\n",
    "#                     # Regular image URL\n",
    "#                     img_data = requests.get(image_url).content\n",
    "#                     img = Image.open(BytesIO(img_data))\n",
    "#                     if img.format != 'GIF':\n",
    "#                         image_pil.append(img)\n",
    "#                         # Process or save the image as needed\n",
    "\n",
    "#     except requests.HTTPError as e:\n",
    "#         print(f\"Skipping URL: {url} - HTTP Error: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing URL: {url} - {e}\")\n",
    "#         # Print more details about the exception for debugging\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Now, you can use the 'images' set as needed.\n",
    "# # For example, you can save the unique images to a folder or display them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import json\n",
    "# import hashlib\n",
    "# import time\n",
    "# from urllib.parse import urlparse, urljoin\n",
    "# from PIL import Image\n",
    "# from torchvision import transforms\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a list to store results\n",
    "# classification_results = []\n",
    "\n",
    "# # Assuming you already have a list of PIL images named 'images'\n",
    "# for i, img in enumerate(image_pil):\n",
    "#     try:\n",
    "#         # Convert the input image to the tensor\n",
    "#         input_tensor = preprocess(img)\n",
    "\n",
    "#         # Convert the input tensor to a batch tensor\n",
    "#         input_batch = torch.unsqueeze(input_tensor, 0)\n",
    "\n",
    "#         # Move the input tensor to the device\n",
    "#         input_batch = input_batch.to(device)\n",
    "\n",
    "#         # Perform binary classification inference\n",
    "#         with torch.no_grad():\n",
    "#             output_binary = model_binary(input_batch)\n",
    "\n",
    "#         # Get the predicted class index for the binary model\n",
    "#         _, predicted_index_binary = torch.max(output_binary, 1)\n",
    "\n",
    "#         # Map the index to the class name for the binary model\n",
    "#         predicted_class_binary = class_names_binary[predicted_index_binary.item()]\n",
    "\n",
    "#         if predicted_class_binary == 'suitable':\n",
    "#             # If binary model predicts 'suitable,' proceed with multiclass model\n",
    "#             # Perform multiclass classification inference\n",
    "#             with torch.no_grad():\n",
    "#                 output_multiclass = model_multiclass(input_batch)\n",
    "\n",
    "#             # Get the predicted class index for the multiclass model\n",
    "#             _, predicted_index_multiclass = torch.max(output_multiclass, 1)\n",
    "\n",
    "#             # Map the index to the class name for the multiclass model\n",
    "#             predicted_class_multiclass = class_names_multiclass[predicted_index_multiclass.item()]\n",
    "\n",
    "#             # Display the image along with its predicted class\n",
    "#             plt.imshow(np.array(img))\n",
    "#             plt.title(f\"Image {i}: Predicted Class - {predicted_class_multiclass}\")\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "\n",
    "#             # Save multiclass result in the list\n",
    "#             classification_results.append({\n",
    "#                 'image_index': i,\n",
    "#                 'predicted_class': predicted_class_multiclass\n",
    "#             })\n",
    "#         else:\n",
    "#             # If binary model predicts 'not suitable,' display the image with its predicted class\n",
    "#             plt.imshow(np.array(img))\n",
    "#             plt.title(f\"Image {i}: Predicted Class - {predicted_class_binary}\")\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "\n",
    "#             # Save binary result in the list\n",
    "#             classification_results.append({\n",
    "#                 'image_index': i,\n",
    "#                 'predicted_class': predicted_class_binary\n",
    "#             })\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing image {i}: {e}\")\n",
    "\n",
    "# # Display classification results\n",
    "# for result in classification_results:\n",
    "#     print(f\"Image {result['image_index']}: Predicted Class - {result['predicted_class']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_LBuXlV98",
  "createdOn": 1705660688957,
  "creationTag": {
   "extendedProperties": {},
   "lastModifiedBy": {
    "login": "aslanovafm"
   },
   "lastModifiedOn": 1705660688957,
   "versionNumber": 0
  },
  "creator": "aslanovafm",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env 39_mortgage)",
   "language": "python",
   "name": "py-dku-venv-39_mortgage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "modifiedBy": "aslanovafm",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
